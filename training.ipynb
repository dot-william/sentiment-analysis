{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re # regex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260097528899452929</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263791921753882624</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>264194578381410304</td>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264041328420204544</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263816256640126976</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>264241571908681728</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#WEB YouTube improves upload process with opti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>264228980444495875</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gonna change my Tumblr theme. I hope I can fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>264210367192915968</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I\\u2019m so jealous of everyone at the Justin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>263737249240342528</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Jim Harbaugh\\u002c Alex Smith Drive Giants Wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>264096818831433728</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#Trending: Tim Tebow is now dating cave woman ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1654 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id sentiment  \\\n",
       "0     260097528899452929   neutral   \n",
       "1     263791921753882624   neutral   \n",
       "2     264194578381410304  negative   \n",
       "3     264041328420204544   neutral   \n",
       "4     263816256640126976   neutral   \n",
       "...                  ...       ...   \n",
       "1649  264241571908681728   neutral   \n",
       "1650  264228980444495875  positive   \n",
       "1651  264210367192915968   neutral   \n",
       "1652  263737249240342528   neutral   \n",
       "1653  264096818831433728   neutral   \n",
       "\n",
       "                                                  tweet  \n",
       "0     Won the match #getin . Plus\\u002c tomorrow is ...  \n",
       "1     Some areas of New England could see the first ...  \n",
       "2     @francesco_con40 2nd worst QB. DEFINITELY Tony...  \n",
       "3     #Thailand Washington - US President Barack Oba...  \n",
       "4     Did y\\u2019all hear what Tony Romo dressed up ...  \n",
       "...                                                 ...  \n",
       "1649  #WEB YouTube improves upload process with opti...  \n",
       "1650  Gonna change my Tumblr theme. I hope I can fin...  \n",
       "1651  I\\u2019m so jealous of everyone at the Justin ...  \n",
       "1652  Jim Harbaugh\\u002c Alex Smith Drive Giants Wor...  \n",
       "1653  #Trending: Tim Tebow is now dating cave woman ...  \n",
       "\n",
       "[1654 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/twitter-2013dev-A.txt', delimiter='\\t', names=['id', 'sentiment', 'tweet'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text will be preprocessed:\n",
    "1. Unicode escape sequence `\\u` will be decoded.\n",
    "2. Punctuations will be removed.\n",
    "3. `#` and `@` will also be removed from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_32748\\1656766160.py:1: DeprecationWarning: invalid escape sequence '\\ '\n",
      "  df['tweet'] = df['tweet'].apply(lambda x: x.encode('utf-8').decode('unicode_escape'))\n",
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_32748\\1656766160.py:1: DeprecationWarning: invalid escape sequence '\\m'\n",
      "  df['tweet'] = df['tweet'].apply(lambda x: x.encode('utf-8').decode('unicode_escape'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260097528899452929</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus, tomorrow is a ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263791921753882624</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>264194578381410304</td>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264041328420204544</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263816256640126976</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y’all hear what Tony Romo dressed up as fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>264241571908681728</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#WEB YouTube improves upload process with opti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>264228980444495875</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gonna change my Tumblr theme. I hope I can fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>264210367192915968</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I’m so jealous of everyone at the Justin Biebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>263737249240342528</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Jim Harbaugh, Alex Smith Drive Giants World Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>264096818831433728</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#Trending: Tim Tebow is now dating cave woman ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1654 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id sentiment  \\\n",
       "0     260097528899452929   neutral   \n",
       "1     263791921753882624   neutral   \n",
       "2     264194578381410304  negative   \n",
       "3     264041328420204544   neutral   \n",
       "4     263816256640126976   neutral   \n",
       "...                  ...       ...   \n",
       "1649  264241571908681728   neutral   \n",
       "1650  264228980444495875  positive   \n",
       "1651  264210367192915968   neutral   \n",
       "1652  263737249240342528   neutral   \n",
       "1653  264096818831433728   neutral   \n",
       "\n",
       "                                                  tweet  \n",
       "0     Won the match #getin . Plus, tomorrow is a ver...  \n",
       "1     Some areas of New England could see the first ...  \n",
       "2     @francesco_con40 2nd worst QB. DEFINITELY Tony...  \n",
       "3     #Thailand Washington - US President Barack Oba...  \n",
       "4     Did y’all hear what Tony Romo dressed up as fo...  \n",
       "...                                                 ...  \n",
       "1649  #WEB YouTube improves upload process with opti...  \n",
       "1650  Gonna change my Tumblr theme. I hope I can fin...  \n",
       "1651  I’m so jealous of everyone at the Justin Biebe...  \n",
       "1652  Jim Harbaugh, Alex Smith Drive Giants World Se...  \n",
       "1653  #Trending: Tim Tebow is now dating cave woman ...  \n",
       "\n",
       "[1654 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].apply(lambda x: x.encode('utf-8').decode('unicode_escape'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. Should we remove the words following @ and #?\n",
    "2. Should we remove numbers? like 2nd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260097528899452929</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus, tomorrow is a ver...</td>\n",
       "      <td>Won the match getin Plus tomorrow is a very bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263791921753882624</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>264194578381410304</td>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "      <td>francesco_con40 2nd worst QB DEFINITELY Tony R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264041328420204544</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "      <td>Thailand Washington US President Barack Obama ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263816256640126976</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y’all hear what Tony Romo dressed up as fo...</td>\n",
       "      <td>Did y all hear what Tony Romo dressed up as fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>264241571908681728</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#WEB YouTube improves upload process with opti...</td>\n",
       "      <td>WEB YouTube improves upload process with optio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>264228980444495875</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gonna change my Tumblr theme. I hope I can fin...</td>\n",
       "      <td>Gonna change my Tumblr theme I hope I can fini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>264210367192915968</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I’m so jealous of everyone at the Justin Biebe...</td>\n",
       "      <td>I m so jealous of everyone at the Justin Biebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>263737249240342528</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Jim Harbaugh, Alex Smith Drive Giants World Se...</td>\n",
       "      <td>Jim Harbaugh Alex Smith Drive Giants World Ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>264096818831433728</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#Trending: Tim Tebow is now dating cave woman ...</td>\n",
       "      <td>Trending Tim Tebow is now dating cave woman fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1654 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id sentiment  \\\n",
       "0     260097528899452929   neutral   \n",
       "1     263791921753882624   neutral   \n",
       "2     264194578381410304  negative   \n",
       "3     264041328420204544   neutral   \n",
       "4     263816256640126976   neutral   \n",
       "...                  ...       ...   \n",
       "1649  264241571908681728   neutral   \n",
       "1650  264228980444495875  positive   \n",
       "1651  264210367192915968   neutral   \n",
       "1652  263737249240342528   neutral   \n",
       "1653  264096818831433728   neutral   \n",
       "\n",
       "                                                  tweet  \\\n",
       "0     Won the match #getin . Plus, tomorrow is a ver...   \n",
       "1     Some areas of New England could see the first ...   \n",
       "2     @francesco_con40 2nd worst QB. DEFINITELY Tony...   \n",
       "3     #Thailand Washington - US President Barack Oba...   \n",
       "4     Did y’all hear what Tony Romo dressed up as fo...   \n",
       "...                                                 ...   \n",
       "1649  #WEB YouTube improves upload process with opti...   \n",
       "1650  Gonna change my Tumblr theme. I hope I can fin...   \n",
       "1651  I’m so jealous of everyone at the Justin Biebe...   \n",
       "1652  Jim Harbaugh, Alex Smith Drive Giants World Se...   \n",
       "1653  #Trending: Tim Tebow is now dating cave woman ...   \n",
       "\n",
       "                                           cleaned_text  \n",
       "0     Won the match getin Plus tomorrow is a very bu...  \n",
       "1     Some areas of New England could see the first ...  \n",
       "2     francesco_con40 2nd worst QB DEFINITELY Tony R...  \n",
       "3     Thailand Washington US President Barack Obama ...  \n",
       "4     Did y all hear what Tony Romo dressed up as fo...  \n",
       "...                                                 ...  \n",
       "1649  WEB YouTube improves upload process with optio...  \n",
       "1650  Gonna change my Tumblr theme I hope I can fini...  \n",
       "1651  I m so jealous of everyone at the Justin Biebe...  \n",
       "1652  Jim Harbaugh Alex Smith Drive Giants World Ser...  \n",
       "1653  Trending Tim Tebow is now dating cave woman fr...  \n",
       "\n",
       "[1654 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove @, #, and punctuation marks from the text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The tweet text.\n",
    "    Returns:\n",
    "        str: The cleaned tweet text.\n",
    "    \"\"\"\n",
    "    # Replace @, #, and non-word characters with a space\n",
    "    # cleaned_text = re.sub(r'[@#]+|\\W', ' ', text)\n",
    "\n",
    "\n",
    "    # Replace @ and the following text, # and the following text, and non-word characters with a space\n",
    "    cleaned_text = re.sub(r'@[\\w]+|#[\\w]+|\\W', ' ', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "df['cleaned_text'] = df['tweet'].apply(clean_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\willi/nltk_data'\n    - 'c:\\\\Users\\\\willi\\\\Desktop\\\\_77\\\\_Activities\\\\venvs\\\\py-acts\\\\nltk_data'\n    - 'c:\\\\Users\\\\willi\\\\Desktop\\\\_77\\\\_Activities\\\\venvs\\\\py-acts\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\willi\\\\Desktop\\\\_77\\\\_Activities\\\\venvs\\\\py-acts\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\willi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\willi\\Desktop\\_77\\_Activities\\Project 1\\sentiment-analysis\\training.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/willi/Desktop/_77/_Activities/Project%201/sentiment-analysis/training.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m word_tokenize\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/willi/Desktop/_77/_Activities/Project%201/sentiment-analysis/training.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtokenized_tweet\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtweet\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(word_tokenize)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/willi/Desktop/_77/_Activities/Project%201/sentiment-analysis/training.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\willi\\Desktop\\_77\\_Activities\\venvs\\py-acts\\lib\\site-packages\\pandas\\core\\series.py:4758\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4756\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4758\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4759\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   4760\u001b[0m         func,\n\u001b[0;32m   4761\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[0;32m   4762\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[0;32m   4763\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   4764\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[0;32m   4765\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\willi\\Desktop\\_77\\_Activities\\venvs\\py-acts\\lib\\site-packages\\pandas\\core\\apply.py:1201\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[0;32m   1200\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1201\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\willi\\Desktop\\_77\\_Activities\\venvs\\py-acts\\lib\\site-packages\\pandas\\core\\apply.py:1281\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[0;32m   1282\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[0;32m   1283\u001b[0m )\n\u001b[0;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1286\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mReturning a DataFrame from Series.apply when the supplied function \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1288\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturns a Series is deprecated and will be removed in a future \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1291\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1292\u001b[0m     )  \u001b[39m# GH52116\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\willi\\Desktop\\_77\\_Activities\\venvs\\py-acts\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[1;32mc:\\Users\\willi\\Desktop\\_77\\_Activities\\venvs\\py-acts\\lib\\site-packages\\pandas\\core\\algorithms.py:1812\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1810\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1812\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[0;32m   1813\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1815\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[0;32m   1816\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\willi\\Desktop\\_77\\_Activities\\venvs\\py-acts\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\willi\\Desktop\\_77\\_Activities\\venvs\\py-acts\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\willi\\Desktop\\_77\\_Activities\\venvs\\py-acts\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\willi\\Desktop\\_77\\_Activities\\venvs\\py-acts\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\willi\\Desktop\\_77\\_Activities\\venvs\\py-acts\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\willi/nltk_data'\n    - 'c:\\\\Users\\\\willi\\\\Desktop\\\\_77\\\\_Activities\\\\venvs\\\\py-acts\\\\nltk_data'\n    - 'c:\\\\Users\\\\willi\\\\Desktop\\\\_77\\\\_Activities\\\\venvs\\\\py-acts\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\willi\\\\Desktop\\\\_77\\\\_Activities\\\\venvs\\\\py-acts\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\willi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "df['tokenized_tweet'] = df['tweet'].apply(word_tokenize)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-acts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
