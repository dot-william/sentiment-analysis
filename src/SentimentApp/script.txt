# Project Script
---
Outline:
- Introduction to the project explaining its purpose and goals.
- Describe the experimental methodology used, including details on how the data was preprocessed and the rationale behind choosing the specific model
- Present the results of the experimentations, including a comparison of performance metrics for each iteration or improvement made
- Conduct demonstration of Django application


### Intro
---
- In today's digital age, understanding public sentiment is paramount for businesses. Real-time feedback from social media, customer reviews, and online discussions offer invaluable insights into public opinion, preferences and concerns. Sentiment analysis, which is a subset of Natural Language Processing, enables us to decode data by discerning the emotional tone behind the words.
- The purpose of this project is to harness the power of sentiment analysis and make it accessible through a web interface. This web application, built using the Django framework, offers users the capability to evaluate the sentiment of any given text. 

- The primary goals of this projects were:
	1. To design a  user-friendly web interface for sentiment analysis
	2. To select and train a sentiment analysis model that provides the highest proficiency in multiclass classification
	3. Integrate the trained model smoothly into the Django application for a seamless user experience.

### Methodology
---
1. **Data Collection**
    - **Source of Data**: We utilized the Twitter dataset provided by our mentors, excluding the sarcasm subset.
    - **Volume of Data**: Of the total 49,593 entries, I observed imbalances: 7,706 negative, 22,240 neutral, and 19,647 positive sentiments. I addressed this with random undersampling.
2. **Data Preprocessing**  
    - **Cleaning**: Initial steps included removing duplicates. Text preprocessing was approached in two primary ways: utilizing the spaCy library and implementing custom functions.
    - In the custom For the custom functions, I preprocessed the text by first eliminating urls, characters, and numbers. However, I also made a function that only removes the @ and # symbols but not remove the words following it, retaining the mentions and hashtags as a way to experiment if doing this can affect the performance when training the model. 
    - Then both of the functions tokenized the text, removed the stop words, and normalized the text by lemmatizing the words to their root form.
    - After preprocessing the text, vectorization was applied using the technique TF-IDF
    - Undersampling was then performed after initially training an SVM model with the preprocessed data from the spaCy package, giving and f1 score of 0.36 for the negative sentiment.
    - After undersampling, the performance was improved to 0.63.
3. **Model Selection**
    - **Training Strategy**: 20% of the data was used for the test set while 80% was used for training
    - **Hyperparameter Tuning**: Gridsearch CV was used to tune the models.
    - Several models were considered for selection: SVM, Decision Trees, Random Forest and Naive Bayes
    - The resulting dataset after undergoing the different preprocessing functions were also used in determining which model provided the best performance 
    - Additionally hyperparameter tuning was applied and these were the following 
    - After several tests, here are the results:
	    - Consistency
		    - There is not a significant variance in performance between the different preprocessing functions of the same model type. This indicates that any modifications or iterations between the preprocessing functions have not drastically impacted overall performance
		    - So the final function to be used was determined by calculating the average execution time for preprocessing functions. According to the chart, preprocess_text_v2 provided the least execution time, which was the function that removed mentions and hashtags. 
		- Model performance
			- **SVM** has the highest accuracy at 0.60, indicating it is the best model among the four for this specific sentiment analysis task on the given dataset.
			- **Decision Trees** have the lowest accuracy at 0.51.
			- **Random Forest** and **Naive Bayes** are close in performance with accuracies of 0.57.
		- **Class-wise Analysis**:
			- **Negative Sentiment**:
			    - SVM and Random Forest perform similarly in terms of precision, while SVM has a slightly higher recall.
			    - Naive Bayes also shows competitive performance for the negative class, while Decision Trees lag behind both in precision and recall.
			- **Neutral Sentiment**:
			    - SVM achieves the highest precision, while Decision Trees have the highest recall. Interestingly, Decision Trees' higher recall comes at a cost of precision, which is the lowest among all models.
			    - Random Forest and Naive Bayes have similar precisions, but Random Forest edges out slightly in recall.
			- **Positive Sentiment**:
			    - SVM leads in precision, while Naive Bayes has the highest recall.
			    - Random Forest's performance is balanced between precision and recall, resulting in an f1-score similar to SVM.
			    - Decision Trees lag in both precision and recall for the positive class.
		- **Comparative Insights**:
			- Looking at the macro averages, SVM's precision, recall, and f1-score are all at 0.60, indicating a balanced performance across all classes.
	- **Rationale for Chosen Model**: 
		- In conclusion, SVM appears to be the most suitable model for this sentiment analysis task on the given dataset. It provided the highest accuracy score of 0.60. 
		- A closer inspection of class-wise metrics reveals that SVM achieves a balanced performance for all sentiment classes - negative, neutral and positive. Balanced performance ensures that the model is versatile and doesn't exhibit a skewed preference for one class over others, which is crucial for sentiment analysis applications.
		- The SVM's consistent scores across precision, recall, and f1-score demonstrate that it's not only performing well but is also likely to be more robust and generalizable to unseen data.
		- While both Random Forest and Naive Bayes have comparable accuracies (both around 0.57), their class-wise metrics aren't as balanced as SVM's. Decision Trees lag in performance in all metrics, indicating potential challenges with the dataset's complexity or intricacies.
		- In conclusion, given the balance in performance, highest accuracy, and consistent metrics across different sentiments, **the Support Vector Machine (SVM) is the best model** for this sentiment analysis task.
		- **Efficiency**: The model might be computationally efficient and provides quick results, crucial for real-time applications.
		- **Capability**: The model might be especially adept at multiclass classification or contextual understanding.

